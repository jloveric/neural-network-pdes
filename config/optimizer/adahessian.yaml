# Second order optimizer
name: adahessian
lr: 1.0
betas: [0.9, 0.999]
eps: 1e-4
weight_decay: 0.0
hessian_power: 1.0
patience: 100
scheduler: plateau # exponential
factor: 0.1
