mlp:
  rotations: 2 # 1 is x,y, 2 is x,y, x-y, x+y, greater includes additional rotations
  style: transform # standard
  nonlinearity: null
  layer_type: "discontinuous"
  n: 2
  n_in: null
  n_out: null
  n_hidden: null
  resnet: False
  # We don't need this if we use max_abs normalization (it seems)
  periodicity: null #2.0

  rescale_output: False
  scale: 2.0
  normalize: maxabs # False, maxabs, midrange, instance
  segments: 3
  input:
    segments: ${mlp.segments}
    width: 2
  output:
    segments: ${mlp.segments}
    width: 3
  hidden:
    segments: ${mlp.segments}
    layers: 4
    width: 20

refinement:
  type: null #p_refine
  epochs: 2
  start_n: 2
  target_n: 7
  step: 1

physics:
  gamma: 1.4
  artificial_viscosity: 0.0

# When this value is 0, no loss reduction occurs with time
# when set to 0, the final state hardly contributes to the loss
time_decay: 0.0

# Inputs are normalized to [-1, 1] so we need to scale the time
# and space derivatives. X is [-1, 1] and t is [0, 1] so t has
# a scale factor of 0.5
scale_x: 1
scale_t: 0.25

# pde weighting
loss_weight:
  # from the paper "Discontinuity Computing Using Physics-Informed Neural Networks"
  discontinuity: 0.0

  interior: 0.1
  boundary: 10
  initial: 10

# Form of equations, conservative or primitive
form: "conservative"
solve_waves: True

# Delta, offset for computing derivatives
delta: 0.01
delta_t: 0.01

# generate this many interior data points
data_size: 10000

max_epochs: 10000
accelerator: cuda
batch_size: 1024

# for discontinuous layers, apply this smoothing factor
# does nothing for continuous or other layers
factor: 0.1

# gradient clipping
gradient_clip: 5.0

# Are you training? Otherwise plot the result
train: True
checkpoint: null
save_images: True

defaults:
  - optimizer: adam # lbfgs
