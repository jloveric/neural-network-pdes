mlp:
  rotations: 2 # 1 is x,y, 2 is x,y, x-y, x+y, greater includes additional rotations
  style: transform # standard
  nonlinearity: null
  layer_type: "discontinuous"
  n: 2
  n_in: null
  n_out: null
  n_hidden: null

  # We don't need this if we use max_abs normalization (it seems)
  periodicity: 2.0

  rescale_output: False
  scale: 2.0
  normalize: True
  segments: 3
  input:
    segments: ${mlp.segments}
    width: 2
  output:
    segments: ${mlp.segments}
    width: 3
  hidden:
    segments: ${mlp.segments}
    layers: 4
    width: 20

physics:
  gamma: 1.4
  artificial_viscosity: 0.0

# When this value is 0, no loss reduction occurs with time
# when set to 0, the final state hardly contributes to the loss
time_decay: 0.0

# Inputs are normalized to [-1, 1] so we need to scale the time
# and space derivatives. X is [-1, 1] and t is [0, 1] so t has
# a scale factor of 0.5
scale_x: 1
scale_t: 0.5

# pde weighting
loss_weight:
  # from the paper "Discontinuity Computing Using Physics-Informed Neural Networks"
  discontinuity: 0.0

  interior: 0.1
  boundary: 10
  initial: 10

# Form of equations, conservative or primitive
form: "conservative"

# Delta, offset for computing derivatives
delta: 0.01
delta_t: 0.01

# generate this many interior data points
data_size: 10000

max_epochs: 10000
gpus: 1
batch_size: 1024

# decay factor for discontinuities
factor: 0.1
gradient_clip: 5.0

# Are you training? Otherwise plot the result
train: True
checkpoint: null
save_images: True

defaults:
  - optimizer: adam # lbfgs
